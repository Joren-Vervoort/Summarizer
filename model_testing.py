# -*- coding: utf-8 -*-
"""model_testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o9EKypgkzeCHWYARH65NdUEWeDU8qPdT

## **Connecting your google drive**
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""## **Importing Libraries**"""

#preprocessing
!pip install text_hammer

#extractive summarizing model
!pip install sumy

#extractive summarizing model
!pip install transformers
!pip install sentencepiece

#metrics
!pip install sent2vec
!pip install sumeval

"""## **Importing Libraries**"""

#Preprocessing

import time
import pandas as pd
import text_hammer as th
import re
import numpy as np
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

#extractive models
## model1
import gensim
from gensim.summarization import summarize


import sumy
import nltk; nltk.download('punkt')
from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser

## model2
from sumy.summarizers.lex_rank import LexRankSummarizer

## model3
from sumy.summarizers.luhn import LuhnSummarizer

## model4
from sumy.summarizers.lsa import LsaSummarizer

## model5
from sumy.summarizers.kl import KLSummarizer

#abstractive models
## model6
from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration

## model7
from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig

## model8
from transformers import GPT2Tokenizer,GPT2LMHeadModel

## model9
from transformers import XLMWithLMHeadModel, XLMTokenizer

#METRICS
from scipy import spatial
from sent2vec.vectorizer import Vectorizer

from sumeval.metrics.rouge import RougeCalculator

"""## **Loading the datasets**"""

#Loading dataset
path = '/content/gdrive/MyDrive/Project_Algorhythm_Summarizer/data'
df = pd.read_csv(f"{path}/news_summary.csv", encoding='latin-1')

#dictionary with contractions that need to be adjusted in the texts/summaries during the next step: "Preprocessing"

contraction_mapping = {"ain't": "is not", "aren't": "are not","can't": "cannot", "'cause": "because", "could've": "could have", "couldn't": "could not",

                           "didn't": "did not", "doesn't": "does not", "don't": "do not", "hadn't": "had not", "hasn't": "has not", "haven't": "have not",

                           "he'd": "he would","he'll": "he will", "he's": "he is", "how'd": "how did", "how'd'y": "how do you", "how'll": "how will", "how's": "how is",

                           "I'd": "I would", "I'd've": "I would have", "I'll": "I will", "I'll've": "I will have","I'm": "I am", "I've": "I have", "i'd": "i would",

                           "i'd've": "i would have", "i'll": "i will",  "i'll've": "i will have","i'm": "i am", "i've": "i have", "isn't": "is not", "it'd": "it would",

                           "it'd've": "it would have", "it'll": "it will", "it'll've": "it will have","it's": "it is", "let's": "let us", "ma'am": "madam",

                           "mayn't": "may not", "might've": "might have","mightn't": "might not","mightn't've": "might not have", "must've": "must have",

                           "mustn't": "must not", "mustn't've": "must not have", "needn't": "need not", "needn't've": "need not have","o'clock": "of the clock",

                           "oughtn't": "ought not", "oughtn't've": "ought not have", "shan't": "shall not", "sha'n't": "shall not", "shan't've": "shall not have",

                           "she'd": "she would", "she'd've": "she would have", "she'll": "she will", "she'll've": "she will have", "she's": "she is",

                           "should've": "should have", "shouldn't": "should not", "shouldn't've": "should not have", "so've": "so have","so's": "so as",

                           "this's": "this is","that'd": "that would", "that'd've": "that would have", "that's": "that is", "there'd": "there would",

                           "there'd've": "there would have", "there's": "there is", "here's": "here is","they'd": "they would", "they'd've": "they would have",

                           "they'll": "they will", "they'll've": "they will have", "they're": "they are", "they've": "they have", "to've": "to have",

                           "wasn't": "was not", "we'd": "we would", "we'd've": "we would have", "we'll": "we will", "we'll've": "we will have", "we're": "we are",

                           "we've": "we have", "weren't": "were not", "what'll": "what will", "what'll've": "what will have", "what're": "what are",

                           "what's": "what is", "what've": "what have", "when's": "when is", "when've": "when have", "where'd": "where did", "where's": "where is",

                           "where've": "where have", "who'll": "who will", "who'll've": "who will have", "who's": "who is", "who've": "who have",

                           "why's": "why is", "why've": "why have", "will've": "will have", "won't": "will not", "won't've": "will not have",

                           "would've": "would have", "wouldn't": "would not", "wouldn't've": "would not have", "y'all": "you all",

                           "y'all'd": "you all would","y'all'd've": "you all would have","y'all're": "you all are","y'all've": "you all have",

                           "you'd": "you would", "you'd've": "you would have", "you'll": "you will", "you'll've": "you will have",

                           "you're": "you are", "you've": "you have"}

#Preprocessing

df = df [["text","ctext"]]
df.columns = ["summary","text"]
df.summary = df.summary.astype('str')
df["text"]  = df["text"].str.replace('?', '"')
df["text"]  = df["text"].str.replace('.', '. ')
df.text = df.text.astype('str')

def pre_process_extractive(df):

  data = df

  def preprocess(text):
      new_text = text
      new_text = re.sub(r'\([^)]*\)', '', new_text)
      new_text = re.sub('"','', new_text)
      new_text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_text.split(" ")])    
      new_text = re.sub(r"'s\b","",new_text)
      return new_text

  text_cleaned = []
  summ_cleaned = []
  for text in data['text']:
      text_cleaned.append(preprocess(text))
  for summary in data['summary']:
      summ_cleaned.append(preprocess(summary))
  clean_df = pd.DataFrame()
  clean_df['summary'] = summ_cleaned
  clean_df['text'] = text_cleaned
  clean_df['summary'].replace('', np.nan, inplace=True)
  clean_df.dropna(axis=0, inplace=True)

  return clean_df


def pre_process_abstractive(df):

  data = df

  StopWords = set(stopwords.words('english'))

  def preprocess(text):
      new_text = text.lower()
      new_text = re.sub(r'\([^)]*\)', '', new_text)
      new_text = re.sub('"','', new_text)
      new_text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_text.split(" ")])    
      new_text = re.sub(r"'s\b","",new_text)
      new_text = ' '.join([word for word in new_text.split() if word not in StopWords])
      new_text = ' '.join([word for word in new_text.split() if len(word) >= 3])
      return new_text

  text_cleaned = []
  summ_cleaned = []
  for text in data['text']:
      text_cleaned.append(preprocess(text))
  for summary in data['summary']:
      summ_cleaned.append(preprocess(summary))
  clean_df = pd.DataFrame()
  clean_df['summary'] = summ_cleaned
  clean_df['text'] = text_cleaned
  clean_df['summary'].replace('', np.nan, inplace=True)
  clean_df.dropna(axis=0, inplace=True)

  return clean_df
print("extracting dataset executing..." )
df_ex = pre_process_extractive(df)
print("done" )

print("abstractive dataset executing..." )
df_ab = pre_process_extractive(df)
print("done" )

"""## **Selecting an article to summarize**"""

article_number = int(input("choose a number between 0 and ... to select an article of the dataset.csv file: " )) #Input for the user to select an article

def choose_article(article_number, df):
  """
  Function that allows the user to select an article and its summary of choice from the dataset (dataset.csv) based on a number.
  
  Input:
  - number : represents the row-number in the dataset.csv file

  Output:
  - article: The full article that needs to be summarized
  - human_summary: A summary made by a human to serve as a benchmark


  """

  #Assigning the article and its summary to variables
  article = df.iloc[article_number]['text']
  human_summary = df.iloc[article_number]['summary']
  return article, human_summary

article_ex, human_summary_ex = choose_article(article_number, df_ex) #Executing the 'choose_article' function
article_ab, human_summary_ab = choose_article(article_number, df_ab) #Executing the 'choose_article' function

print(article_ex)
print(article_ab)

"""Extractive models"""

start = time.time()

class extractive_models:

  def run_models(df, article):

    #GENSIM MODEL(S)

    #Model 1: TextRank
    start1 = time.time()
    model1=summarize(article, ratio=0.1, word_count=80) 
    end1 = time.time()
    time1 = (end1 - start1)

    #SUMY MODEL(S)
    start_sumy = time.time()
    parser=PlaintextParser.from_string(article,Tokenizer('english')) # Creating the parser
    end_sumy = time.time()
    time_sumy = (end_sumy - start_sumy)

    #Model 2: LexRank
    start2 = time.time()
    lex_rank_summarizer = LexRankSummarizer()
    lexrank_summary = lex_rank_summarizer(parser.document,sentences_count=3)

    model2 = ""

    for sentence in lexrank_summary:
      sentence = str(sentence)
      model2 += " " + sentence
    end2 = time.time()
    time2 = (end2 - start2 + time_sumy)

    #Model 3: Luhn
    start3 = time.time()
    luhn_summarizer=LuhnSummarizer()
    luhn_summary=luhn_summarizer(parser.document,sentences_count=3)

    model3 = ""

    for sentence in luhn_summary:
      sentence = str(sentence)
      model3 += " " + sentence
    end3 = time.time()
    time3 = (end3 - start3 + time_sumy)

    #Model 4: LSA (Latent Sementic Analysis)
    start4 = time.time()
    lsa_summarizer=LsaSummarizer()
    lsa_summary= lsa_summarizer(parser.document,3)

    model4 = ""

    for sentence in lsa_summary:
      sentence = str(sentence)
      model4 += " " + sentence
    end4 = time.time()
    time4 = (end4 - start4 + time_sumy)

    #Model 5: KL-Sum
    start5 = time.time()
    kl_summarizer=KLSummarizer()
    kl_summary=kl_summarizer(parser.document,sentences_count=3)

    model5 = ""

    for sentence in kl_summary:
      sentence = str(sentence)
      model5 += " " + sentence
    end5 = time.time()
    time5 = (end5 - start5 + time_sumy)

    return model1, model2, model3, model4, model5, time1, time2, time3, time4, time5

#Executing the Functions

em = extractive_models
model1, model2, model3, model4, model5, time1, time2, time3, time4, time5 = em.run_models(df_ex,article_ex)


end = time.time()
print(end - start)

"""Abstractive models"""

start = time.time()

class abstractive_models:
  """
  def run_t5(df, article):
    print("model6")
    #Model 6: T5-transformer
    start6 = time.time()

    # Instantiating the model and tokenizer 
    my_model = T5ForConditionalGeneration.from_pretrained('t5-base')
    tokenizer = T5Tokenizer.from_pretrained('t5-base')

    # encoding the input text
    input_ids = tokenizer.encode(article, return_tensors='pt', max_length=150, truncation=True)

    # Generating summary ids
    summary_ids = my_model.generate(input_ids)

    # Decoding the tensor and printing the summary.
    model6= tokenizer.decode(summary_ids[0])

    end6 = time.time()
    time6 = (end6 - start6)

    return model6, time6
  """
  def run_bart(df, article):
    print("model7")
    #Model 7: BART-transformer
    start7 = time.time()
    
    # Loading the model and tokenizer for bart-large-cnn

    tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')
    model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

    # Encoding the inputs and passing them to model.generate()
    inputs = tokenizer.batch_encode_plus([article],return_tensors='pt')
    summary_ids = model.generate(inputs['input_ids'], early_stopping=True)

    # Decoding and printing the summary
    model7 = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    end7 = time.time()
    time7 = (end7 - start7)

    return model7, time7

  def run_gpt2(df, article):
    print("model8")
    #Model 8: GPT-2-transformer
    start8 = time.time()
    
    # Instantiating the model and tokenizer with gpt-2
    tokenizer=GPT2Tokenizer.from_pretrained('gpt2')
    model=GPT2LMHeadModel.from_pretrained('gpt2')

    # Encoding text to get input ids & pass them to model.generate()
    inputs=tokenizer.batch_encode_plus([article],return_tensors='pt',max_length=150, truncation=True)
    summary_ids=model.generate(inputs['input_ids'],early_stopping=True)

    # Decoding and printing summary
    model8=tokenizer.decode(summary_ids[0],skip_special_tokens=True)

    end8 = time.time()
    time8 = (end8 - start8)

    return model8, time8

  def run_xlm(df, article):
    print("model9")
    #Model 9: XLM-transformer
    start9 = time.time()
    
    # Instantiating the model and tokenizer 

    tokenizer=XLMTokenizer.from_pretrained('xlm-mlm-en-2048')
    model=XLMWithLMHeadModel.from_pretrained('xlm-mlm-en-2048')

    # Encoding text to get input ids & pass them to model.generate()
    inputs=tokenizer.batch_encode_plus([article],return_tensors='pt',max_length=150,truncation=True)
    summary_ids=model.generate(inputs['input_ids'],early_stopping=True)

    # Decode and print the summary
    model9=tokenizer.decode(summary_ids[0],skip_special_tokens=True)

    end9 = time.time()
    time9 = (end9 - start9)

    return model9, time9


#Executing the Functions

am = abstractive_models
#model6, time6 = am.run_t5(df_ab, article_ab)
model7, time7 = am.run_bart(df_ab, article_ab)
model8, time8 = am.run_gpt2(df_ab, article_ab)
model9, time9 = am.run_xlm(df_ab, article_ab)

end = time.time()
print(end - start)

"""- T5 transformer

from transformers import pipeline

# use bart in pytorch
summarizer = pipeline("summarization")
bart_summary = summarizer(article, max_length = 6000, min_length=5, do_sample=False)

# use t5 in tf
summarizer = pipeline("summarization", model="t5-base", framework="tf")
t5_summary = summarizer(article, max_length = 6000, min_length=5, do_sample=False)

model6 = bart_summary
model7 = t5_summary

model6 = str(bart_summary[0]['summary_text'])
model7 = str(t5_summary[0]['summary_text'])

def word_embedding_cosine_spatial_distance():
  sentences = [
      human_summary,model7
  ]

  vectorizer = Vectorizer()
  vectorizer.bert(sentences)
  vectors_bert = vectorizer.vectors



  dist_7 = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])


  print("------------------------")
  print("------------------------")
  print("All model scores:")
  print(f'model7: {dist_7}')
  print("------------------------")
  print("------------------------")

word_embedding_cosine_spatial_distance() #Execution of the "word_embedding_cosine_spatial_distance" function

## **METRICS**

- Word embedding (BERT) + cosine spatial distance
"""

human_summary = human_summary_ex

print(f"article: {article_number}")

def word_embedding_cosine_spatial_distance():
  sentences = [
      human_summary,model1,model2,model3,model4,model5,model7,model8,model9
  ] #model6 is taken out

  vectorizer = Vectorizer()
  vectorizer.bert(sentences)
  vectors_bert = vectorizer.vectors


  dist_1 = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])
  dist_2 = spatial.distance.cosine(vectors_bert[0], vectors_bert[2])
  dist_3 = spatial.distance.cosine(vectors_bert[0], vectors_bert[3])
  dist_4 = spatial.distance.cosine(vectors_bert[0], vectors_bert[4])
  dist_5 = spatial.distance.cosine(vectors_bert[0], vectors_bert[5])
  #dist_6 = spatial.distance.cosine(vectors_bert[0], vectors_bert[6])
  dist_7 = spatial.distance.cosine(vectors_bert[0], vectors_bert[6])
  dist_8 = spatial.distance.cosine(vectors_bert[0], vectors_bert[7])
  #dist_9 = spatial.distance.cosine(vectors_bert[0], vectors_bert[8])

  best_score = min(dist_1, dist_2,dist_3,dist_4, dist_5,dist_7,dist_8,dist_9) #dist6 is taken out
  list_scores = [dist_1, dist_2,dist_3,dist_4, dist_5,dist_7,dist_8,dist_9] #dist6 is taken out

  n = 0

  for i in list_scores:
    n += 1
    if best_score == i:
      if n<=5:
        print("------------------------")
        print("------------------------")
        print(f"The best model is MODEL{n} with a score of {i}")
      else:
        print("------------------------")
        print("------------------------")
        print(f"The best model is MODEL{n+1} with a score of {i}")
      break
    else:
      continue

  print("------------------------")
  print("------------------------")
  print("All model scores:")
  print('model1: {0}, model2: {1}, model3:{2}, model4: {3}, model5: {4}, model7: {5}, model8:{6}'.format(dist_1, dist_2,dist_3,dist_4, dist_5,dist_7,dist_8)) #dist6 is taken out
  print("------------------------")
  print("------------------------")

word_embedding_cosine_spatial_distance() #Execution of the "word_embedding_cosine_spatial_distance" function

"""- ROUGE"""

summaries = [model1,model2,model3,model4,model5,model7,model8]

rouge = RougeCalculator(stopwords=True, lang="en")

def rouge_calc(preds, targets):
    rouge_1 = [rouge.rouge_n(summary=preds[i],references=targets,n=1) for i in range(len(preds))]
    rouge_2 = [rouge.rouge_n(summary=preds[i],references=targets,n=2) for i in range(len(preds))]
    rouge_l = [rouge.rouge_l(summary=preds[i],references=targets) for i in range(len(preds))]

    return {"Rouge_1": np.array(rouge_1),
            "Rouge_2": np.array(rouge_2),
            "Rouge_L": np.array(rouge_l)}

rouge_calc(summaries, human_summary)

"""- Human control"""

human_summary



print(f"model1:{model1}")
print(f"model2:{model2}")
print(f"model3:{model3}")
print(f"model4:{model4}")
print(f"model5:{model5}")
#print(f"model6:{model6}")
print(f"model7:{model7}")
print(f"model8:{model8}")
print(f"model9:{model9}")







"""## APP-stuff

from flask_ngrok import run_with_ngrok
from flask import Flask, render_template , request, url_for, redirect 
import os

##Running the flask app
app = Flask(__name__, template_folder='/content/gdrive/MyDrive/Project_Algorhythm_Summarizer/templates')

#start ngrok when app is run
run_with_ngrok(app)

@app.route("/")
def summarizer():
  if request.method == "POST":
    text = request.form["txt"]
    return redirect(url_for("summary", txt=text))
  else:
    return render_template("index.html")

@app.route("/summarized")
def summary(txt):
  summary_of_txt = summarizer(txt)
  return f"<p>{summary_of_txt}</p>"



if __name__ == '__main__':
  app.run()

from flask_ngrok import run_with_ngrok
from flask import Flask, render_template , request, url_for, redirect 
import os

##Running the flask app
app = Flask(__name__, template_folder='/content/gdrive/MyDrive/Project_Algorhythm_Summarizer/templates')

#start ngrok when app is run
run_with_ngrok(app)

@app.route("/")
def home():
  return render_template("index.html")

@app.route("/login", methods=["POST","GET"])
def login():
  if request.method == "POST":
    user = request.form["nm"]
    return redirect(url_for("user", usr=user))
  else:
    return render_template("login.html")

@app.route("/<usr>")
def user(usr):
  return f'<h1>{usr}</h1>'

if __name__ == '__main__':
  app.run()
"""







